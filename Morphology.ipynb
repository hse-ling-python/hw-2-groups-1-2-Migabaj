{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание на тему \"Морфология\", часть 1\n",
    "\n",
    "## Короткое введение\n",
    "\n",
    "Для начала встроим проверку PEP-8, чтобы код был красивым:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем несколько важных библиотек, которые нам понадобятся на протяжении всей работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import json\n",
    "import collections as c\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем Mystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И облегчим себе жизнь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откроем текст, распарсим его при помощи Mystem и посчитаем время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 789 ms, sys: 186 ms, total: 976 ms\n",
      "Wall time: 7.48 s\n"
     ]
    }
   ],
   "source": [
    "with open('Luzhin.txt', encoding='utf-8') as f:\n",
    "    book = f.read()\n",
    "%time ana = m.analyze(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим новый файл, где будут лежать jsonы. Чтобы файл выглядел красиво, как набор твитов из предыдущей домашки, разделим их на строчки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Luzhin_mystem.json', 'w', encoding='utf-8') as f:\n",
    "    for word in ana:\n",
    "        if 'analysis' in word:\n",
    "            json.dump(word, f, ensure_ascii=False)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура!\n",
    "\n",
    "## Задание 2\n",
    "\n",
    "Импортируем nltk и pymorphy2. Из них возьмем нужные нам модули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизируем текст при помощи nltk и замеряем время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 845 ms, sys: 15.3 ms, total: 860 ms\n",
      "Wall time: 919 ms\n"
     ]
    }
   ],
   "source": [
    "%time tokens = word_tokenize(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, на этом моменте началось что-то странное: %time влиял на дальнейшую работу файла и в том числе мешал работать функции print(). Поэтому замерил время я другим образом.\n",
    "\n",
    "Итак, парсим каждое слово, добавляем его в новый список и замеряем время работы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.936647176742554\n"
     ]
    }
   ],
   "source": [
    "words_parsed = []\n",
    "start = time.time()\n",
    "for token in tokens:\n",
    "    par = morph.parse(token)\n",
    "    words_parsed.append(par)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем словари, в котором есть само слово и список вариантов и их свойств. Добавляем все эти словари в один список:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "allthemwords = []\n",
    "for word in words_parsed:\n",
    "    word_dict = {}\n",
    "    for var in word:\n",
    "        if var == word[0]:\n",
    "            word_dict['word'] = var.word\n",
    "            word_dict['vars'] = []\n",
    "        var_dict = {}\n",
    "        var_dict['tag'] = str(var.tag).split(',')\n",
    "        var_dict['normal_form'] = var.normal_form\n",
    "        var_dict['score'] = var.score\n",
    "        word_dict['vars'].append(var_dict)\n",
    "    allthemwords.append(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем новый файл, куда дампаем все эти словари."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Luzhin_pymorphy.json', 'w', encoding='utf-8') as f:\n",
    "    for word in allthemwords:\n",
    "        json.dump(word, f, ensure_ascii=False)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3\n",
    "\n",
    "Чтобы не повторяться, создадим две функции.\n",
    "\n",
    "Первая берет текст, проанализированный при помощи Mystem, и если часть речи совпадает с заданной, то единица прибавляется к числу count. К числу all_count единица прибавляется каждый раз, когда \"слово\" не пустое, т.е. там есть ключ 'analysis'. В итоге мы получаем долю слов, которую составляет заданная часть речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(s):\n",
    "    all_count = 0\n",
    "    count = 0\n",
    "    for word in ana:\n",
    "        if 'analysis' in word and word['analysis']:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            if pos == s:\n",
    "                count += 1\n",
    "            all_count += 1\n",
    "    return (count / all_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вторая функция составляет топ-n список самых популярных слов определенной части речи. Мы выуживаем название части речи из закопанного ключа gr и, если часть речи совпадает с заданной, добавляем в список лексему. Из списка потом мы составляем счетчик и печатаем n самых популярных лексем и к каждой лексеме -- отношение ко всем словам в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top(num, s):\n",
    "    all_count = 0\n",
    "    s_list = []\n",
    "    for word in ana:\n",
    "        if 'analysis' in word and len(word['analysis']) > 0:\n",
    "            gr = word['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            if pos == s:\n",
    "                s_list.append(word['analysis'][0]['lex'])\n",
    "            all_count += 1\n",
    "    s_count = c.Counter(s_list)\n",
    "    i = 1\n",
    "    for s in s_count.most_common(num):\n",
    "        print(str(i) + '.', s[0] + ':', s[1] / all_count)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сначала создаем список из всех частей речи, которые есть в тексте, и потом -- для каждой из них печатаем получившееся отношение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV: 0.07462339774619525\n",
      "SPRO: 0.07981257018936606\n",
      "V: 0.1999767648995082\n",
      "CONJ: 0.08418851411532355\n",
      "PR: 0.10480966580180459\n",
      "S: 0.25426944971537\n",
      "APRO: 0.04465011811176083\n",
      "A: 0.09013282732447818\n",
      "PART: 0.03729233628935445\n",
      "ADVPRO: 0.021821631878557873\n",
      "INTJ: 0.0008906788521860357\n",
      "NUM: 0.005111722108198118\n",
      "ANUM: 0.002420322967896836\n"
     ]
    }
   ],
   "source": [
    "all_pos = []\n",
    "for word in ana:\n",
    "    if 'analysis' in word and len(word['analysis']) > 0:\n",
    "        gr = word['analysis'][0]['gr']\n",
    "        pos = gr.split('=')[0].split(',')[0]\n",
    "        if pos not in all_pos:\n",
    "            all_pos.append(pos)\n",
    "for pos in all_pos:\n",
    "    print(pos + ':', ratio(pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Топ-списки теперь сделать довольно легко при помощи нашей функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-20 глаголов:\n",
      "1. быть: 0.016651822019130232\n",
      "2. сказать: 0.006699453975138442\n",
      "3. становиться: 0.0028850249777330286\n",
      "4. говорить: 0.0024590481353831857\n",
      "5. мочь: 0.0023428726329241373\n",
      "6. знать: 0.0018588080393447703\n",
      "7. сидеть: 0.0014909189482244511\n",
      "8. находить: 0.0014909189482244511\n",
      "9. играть: 0.0014134686132517523\n",
      "10. спрашивать: 0.0013941060295085776\n",
      "11. выходить: 0.001374743445765403\n",
      "12. глядеть: 0.0012972931107927041\n",
      "13. продолжать: 0.0012779305270495295\n",
      "14. думать: 0.0012004801920768306\n",
      "15. стоять: 0.0011423924408473067\n",
      "16. смотреть: 0.0011423924408473067\n",
      "17. понимать: 0.0010843046896177825\n",
      "18. увидеть: 0.0010649421058746078\n",
      "19. оказываться: 0.0010455795221314332\n",
      "20. стараться: 0.0010262169383882585\n",
      "\n",
      "Топ-20 наречий:\n",
      "1. еще: 0.0028850249777330286\n",
      "2. очень: 0.0024590481353831857\n",
      "3. уже: 0.0021298842117492157\n",
      "4. опять: 0.001936258374317469\n",
      "5. вдруг: 0.0018975332068311196\n",
      "6. теперь: 0.0016458196181698486\n",
      "7. сразу: 0.0012779305270495295\n",
      "8. хорошо: 0.0010843046896177825\n",
      "9. быстро: 0.0010649421058746078\n",
      "10. много: 0.0009681291871587344\n",
      "11. далеко: 0.0009100414359292104\n",
      "12. наконец: 0.0008325911009565116\n",
      "13. совершенно: 0.000813228517213337\n",
      "14. нужно: 0.0007357781822406382\n",
      "15. сейчас: 0.0006970530147542888\n",
      "16. тихо: 0.0006776904310111142\n",
      "17. иногда: 0.0006583278472679394\n",
      "18. затем: 0.0006583278472679394\n",
      "19. надо: 0.0006389652635247647\n",
      "20. рядом: 0.0006002400960384153\n"
     ]
    }
   ],
   "source": [
    "print('Топ-20 глаголов:')\n",
    "top(20, 'V')\n",
    "print()\n",
    "print('Топ-20 наречий:')\n",
    "top(20, 'ADV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4\n",
    "\n",
    "Создаем список токенов без ненужных знаков препинания и приводим к нижнему регистру. Создаем список всех биграмм, а из них -- счетчик. Дальше пользуемся функцией most_common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и в: 73\n",
      "сказал лужин: 58\n",
      "что он: 53\n",
      "лужин и: 49\n",
      "и не: 45\n",
      "у него: 45\n",
      "и он: 42\n",
      "и с: 42\n",
      "как будто: 41\n",
      "сказал он: 40\n",
      "и все: 38\n",
      "и лужин: 36\n",
      "и вдруг: 35\n",
      "все это: 34\n",
      "он не: 34\n",
      "сказала она: 34\n",
      "то что: 32\n",
      "глядя на: 30\n",
      "так как: 29\n",
      "с ним: 28\n",
      "о том: 28\n",
      "не было: 28\n",
      "а потом: 28\n",
      "и что: 27\n",
      "и на: 26\n"
     ]
    }
   ],
   "source": [
    "tokens = [w.lower() for w in word_tokenize(book) if w.isalpha()]\n",
    "bgrm = nltk.bigrams(tokens)\n",
    "bgrms = list(map(' '.join, bgrm))\n",
    "bgrms_counter = c.Counter(bgrms)\n",
    "for answer in bgrms_counter.most_common(25):\n",
    "    print(answer[0] + ':', answer[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То, что самая популярная биграмма: \"и в\" -- неудивительно. Оба слова -- стоп-слова, оба очень частотные. Но интересно, что \"сказал лужин\" состоит не из стоп-слов. Тем временем ни жена, ни мать жены, ни Валентинов не встречаются в этих биграммах. Возможно, Лужин говорит чаще остальных в романе. Но может быть Набоков просто любит упоминать его имя: слово \"лужин\" встречается в еще нескольких биграммах этого списка.\n",
    "\n",
    "Практически такая же история (если говорить о коде) с триграммами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "сказал лужин и: 24\n",
      "о том что: 16\n",
      "на следующий день: 10\n",
      "сказал он и: 9\n",
      "сказала она и: 8\n",
      "с тех пор: 7\n",
      "в первый раз: 6\n",
      "на то что: 6\n",
      "о том как: 6\n",
      "что все это: 6\n",
      "ему показалось что: 6\n",
      "в том что: 5\n",
      "сказал лужин старший: 5\n",
      "как только он: 5\n",
      "тех пор как: 5\n",
      "в это мгновение: 5\n",
      "ни с того: 5\n",
      "с того ни: 5\n",
      "того ни с: 5\n",
      "ни с сего: 5\n",
      "у себя в: 5\n",
      "с этого дня: 5\n",
      "после того как: 5\n",
      "не зная что: 5\n",
      "у него было: 5\n"
     ]
    }
   ],
   "source": [
    "trgrm = nltk.trigrams(tokens)\n",
    "trgrms = list(map(' '.join, trgrm))\n",
    "trgrms_counter = c.Counter(trgrms)\n",
    "for answer in trgrms_counter.most_common(25):\n",
    "    print(answer[0] + ':', answer[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут Лужин вообще встает на первое место. К тому же, 5 раз встречается \"сказал Лужин-старший\" (отец Лужина), что несомненно повлияло на частотность биграммы \"сказал Лужин\". Остальные триграммы особо ничем не выделяются: многие из них входят в список топ-100 самых частотных триграмм в НКРЯ.\n",
    "\n",
    "*Миша Сонькин*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
